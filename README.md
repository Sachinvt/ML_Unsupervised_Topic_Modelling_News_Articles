# ML_Unsupervised_Topic_Modelling_News_Articles
![image](https://github.com/Sachinvt/ML_Unsupervised_Topic_Modelling_News_Articles/assets/140580938/5ea9c55f-c489-4d21-998c-01d852d30005)

# Project Summary -
The project focuses on the BBC News dataset, containing news articles from 2004-2005 across five categories: Business, Entertainment, Politics, Sport, and Tech. The primary goal is to perform topic modeling to uncover latent themes and topics within the dataset. Two common topic modeling algorithms, Latent Dirichlet Allocation (LDA) and Latent Semantic Analysis (LSA), are employed for this purpose.

Initially, the dataset is consolidated into a DataFrame with columns for the article's Title, Description, and Category. The Description column undergoes pre-processing, including contraction expansion, punctuation and digit removal, whitespace cleanup, and stop word removal. Lemmatization is also applied to group words into their lemma forms. The pre-processed data is then vectorized using CountVectorizer from scikit-learn. LDA is used for topic modeling, with hyperparameter tuning to identify the underlying topics in the dataset.

Additionally, the LDA model is re-applied using TfidfVectorizer for vectorization to compare accuracy. The project establishes a null hypothesis that CountVectorizer provides more accurate results for the LDA model than TfidfVectorizer. Furthermore, an LSA model is applied to tokenized inputs using both CountVectorizer and TfidfVectorizer. LSA leverages truncated singular value decomposition (SVD) to reduce dimensionality and create a topic-term matrix, representing term weights within topics.

After identifying latent topics, the models are evaluated by comparing them to the original article categories using metrics such as Accuracy, individual topic precision, recall, and F1 scores. Word clouds are generated for each topic to visualize the most frequent words in each topic and assess their relevance.

The evaluation reveals that the LDA model with CountVectorizer achieves the highest accuracy, approximately 93%. The F1 score is highest for Sport (97%) and lowest for Politics (around 90%). The LDA model with TfidfVectorizer has lower accuracy, potentially due to an over-determination of the Entertainment topic, resulting in low precision. The LSA model exhibits significantly lower accuracy, largely due to over-determination of one topic.

For future work, the project suggests refining stop word removal to further reduce dimensionality and tuning additional hyperparameters to enhance model performance. It also mentions the possibility of exploring other topic modeling algorithms and investigating topic evolution. This summary encapsulates the key aspects and findings of the project, emphasizing the choice of topic modeling algorithms, data preprocessing, model evaluation, and directions for future research.
# Problem Statement
The project aims to perform topic modeling on a dataset of BBC News articles, with the goal of uncovering latent themes and topics within the text. Using Latent Dirichlet Allocation (LDA) and Latent Semantic Analysis (LSA) algorithms, the project explores different text preprocessing techniques and vectorization methods to determine the most effective approach for topic modeling. Evaluation metrics are employed to assess the accuracy of the models and their alignment with the original article categories. The project also highlights opportunities for future work, including refining stop word removal and exploring alternative topic modeling algorithms.

# Conclusion
The project involved a comprehensive analysis of text data, specifically news articles, with the aim of uncovering underlying topics. Several noteworthy conclusions were drawn during the project:

Data Parsing Challenges: The initial stages of the project encountered encoding errors such as UnicodeError and ParserError when reading news articles in text file format. To address this, effective exception handling was implemented to ensure proper data reading. This ensured that the text data could be processed successfully.

Textual Pre-processing: The text data underwent a series of pre-processing steps to prepare it for topic modeling. During this phase, it became evident that further refinement was possible. For instance, additional stopwords could be removed to optimize the model further. Certain words like "use" and "go" may not contribute significantly to topic definition. Additionally, handling words like "us" became essential, as they can have multiple meanings, including "United States."

Stemming vs. Lemmatization: The choice of lemmatization over stemming for textual pre-processing was a strategic decision. Unlike stemming, which simply shortens words to their root form, lemmatization considers the context and morphology of words. This approach preserves the original meaning as much as possible, which is crucial in topic modeling. Topics often rely on the subtle nuances of language and word context, making lemmatization a preferred method to uncover these topics effectively.

Null Hypotheses for Vectorization: A critical step in the project was establishing a null hypothesis to determine the optimal vectorization technique for data tokenization. CountVectorizer and TFIDFVectorizer were considered, and the results favored CountVectorizer. This vectorization method is well-suited for LDA models, which rely on modeling word count distributions in documents and topic distributions across documents. CountVectorizer converts text into word count arrays, an ideal format for LDA models.

LDA vs. LSA Models: After implementing both Latent Dirichlet Allocation (LDA) and Latent Semantic Analysis (LSA) models, a clear preference emerged for the LDA model. The LDA model, especially when used in conjunction with CountVectorizer for tokenization, outperformed LSA in categorizing the underlying topics in the news articles. The LDA model achieved a remarkable model accuracy of 93%, indicating its effectiveness in uncovering meaningful topics. Examining word distributions for each topic facilitated the easy identification of correlations between frequent words and topics.

In summary, the project addressed various data parsing challenges, refined textual pre-processing, and made strategic choices in terms of vectorization techniques and text processing methods. The final preference for the LDA model, driven by its high accuracy and interpretability, highlighted its effectiveness in uncovering latent topics within the corpus of news articles. These findings offer valuable insights for future projects involving topic modeling and text analysis.
